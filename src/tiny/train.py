import os
from transformers import RobertaTokenizer, AutoModelForSequenceClassification, Trainer, TrainingArguments
from transformers import BertForSequenceClassification, BertTokenizer
from datasets import Dataset
from sklearn.model_selection import train_test_split
import torch
import logging
from datetime import datetime
import collect

# ---------------------
# æ—¥å¿—è®¾ç½®
# ---------------------

current_time = datetime.now().strftime("%Y%m%d_%H%M%S")
os.makedirs("./logs", exist_ok=True)

logging.basicConfig(level=logging.INFO, format="%(levelname)s: %(message)s", filename=f"./logs/tinybert-train-{current_time}.log", filemode="w")

# ---------------------
# æ•°æ®å‡†å¤‡
# ---------------------

logging.info("ğŸ“ Loading dataset...")
df = collect.label_data()
df = df.sample(frac=1).reset_index(drop=True)  # shuffle

# åˆ é™¤ç©ºä»£ç 
df = df.dropna(subset=["code"])
df["code"] = df["code"].astype(str)
df["label"] = df["label"].astype("int64")  # ğŸ‘ˆ å¼ºåˆ¶è½¬æ¢ä¸ºæ•´æ•°

# åˆ’åˆ†æ•°æ®é›†
train_df, test_df = train_test_split(df, test_size=0.2, stratify=df['label'], random_state=42)
train_df, val_df = train_test_split(train_df, test_size=0.1, stratify=train_df['label'], random_state=42)

logging.info(f"âœ… Train size: {len(train_df)} | Val: {len(val_df)} | Test: {len(test_df)}")

# ---------------------
# åŠ è½½ tokenizer & æ•°æ®é›†
# ---------------------

teacher_tokenizer = RobertaTokenizer.from_pretrained("microsoft/codebert-base")
student_tokenizer = BertTokenizer.from_pretrained("huawei-noah/TinyBERT_General_4L_312D")

def tokenize_teacher(example):
    return teacher_tokenizer(example["code"], padding="max_length", truncation=True, max_length=512)

def tokenize_student(example):
    return student_tokenizer(example["code"], padding="max_length", truncation=True, max_length=512)

train_ds_teacher = Dataset.from_pandas(train_df).map(tokenize_teacher, batched=True)
val_ds_teacher = Dataset.from_pandas(val_df).map(tokenize_teacher, batched=True)

train_ds_student = Dataset.from_pandas(train_df).map(tokenize_student, batched=True)
val_ds_student = Dataset.from_pandas(val_df).map(tokenize_student, batched=True)

train_ds_teacher.set_format(type="torch", columns=["input_ids", "attention_mask", "label"])
val_ds_teacher.set_format(type="torch", columns=["input_ids", "attention_mask", "label"])

train_ds_student.set_format(type="torch", columns=["input_ids", "attention_mask", "label"])
val_ds_student.set_format(type="torch", columns=["input_ids", "attention_mask", "label"])


# ---------------------
# åŠ è½½æ¨¡å‹
# ---------------------

logging.info("ğŸ§  Loading teacher and student models...")
teacher_model = AutoModelForSequenceClassification.from_pretrained("../full/codebert_model", num_labels=2)
student_model = BertForSequenceClassification.from_pretrained("huawei-noah/TinyBERT_General_4L_312D", num_labels=2)

# ---------------------
# è’¸é¦è®­ç»ƒï¼ˆéœ€è¦è‡ªå®šä¹‰ Trainerï¼‰
# ---------------------

from transformers import TrainerCallback

class DistillationTrainer(Trainer):
    def __init__(self, teacher_model=None, *args, **kwargs):
        super().__init__(*args, **kwargs)
        self.teacher_model = teacher_model.to(self.args.device)  # è‡ªåŠ¨åŒ¹é… Trainer ä½¿ç”¨çš„ device
        self.teacher_model.eval()  # ç¦ç”¨ dropout

    def compute_loss(self, model, inputs, return_outputs=False, **kwargs):
        # å– logits
        student_outputs = model(**inputs)
        with torch.no_grad():
            teacher_outputs = self.teacher_model(input_ids=inputs["input_ids"],
                                            attention_mask=inputs["attention_mask"])
        student_logits = student_outputs.logits
        teacher_logits = teacher_outputs.logits

        # KD lossï¼šKLæ•£åº¦
        loss_fct = torch.nn.KLDivLoss(reduction="batchmean")
        student_log_softmax = torch.nn.functional.log_softmax(student_logits / 2.0, dim=-1)
        teacher_softmax = torch.nn.functional.softmax(teacher_logits / 2.0, dim=-1)
        kd_loss = loss_fct(student_log_softmax, teacher_softmax) * (2.0 ** 2)

        # ç›‘ç£ loss
        ce_loss_fct = torch.nn.CrossEntropyLoss()
        ce_loss = ce_loss_fct(student_logits.view(-1, 2), inputs["labels"].view(-1))

        # æ€» loss
        loss = kd_loss + ce_loss
        return (loss, student_outputs) if return_outputs else loss

# ---------------------
# è®­ç»ƒå‚æ•°
# ---------------------

training_args = TrainingArguments(
    output_dir="./tinybert_results",
    logging_dir="./logs",
    logging_steps=50,
    eval_strategy="epoch",
    save_strategy="epoch",
    load_best_model_at_end=True,
    learning_rate=5e-5,
    per_device_train_batch_size=16,
    per_device_eval_batch_size=16,
    num_train_epochs=6,
    weight_decay=0.01,
    fp16=torch.cuda.is_available(),
    report_to="tensorboard"
)

# ---------------------
# Trainer
# ---------------------

trainer = DistillationTrainer(
    model=student_model,
    args=training_args,
    train_dataset=train_ds_student,
    eval_dataset=val_ds_student,
    teacher_model=teacher_model  # æ˜¾å¼ä¼ å…¥
)

# ---------------------
# è®­ç»ƒ
# ---------------------

logging.info("ğŸš€ Training TinyBERT with KD...")
trainer.train()

# ---------------------
# ä¿å­˜æ¨¡å‹
# ---------------------

logging.info("ğŸ’¾ Saving student model...")
trainer.save_model("tinybert_student_model")
student_tokenizer.save_pretrained("tinybert_student_model")

logging.info("âœ… Done! You can now run: tensorboard --logdir=./logs")
